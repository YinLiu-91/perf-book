---
typora-root-url: ..\..\img
---

## Instruction Set Architecture

指令集是软件用来与硬件通信的词汇。指令集架构 (ISA) 定义了软件和硬件之间的契约。 Intel x86、ARM v8、RISC-V 是当今最广泛部署的 ISA 示例。所有这些都是 64 位架构，即所有地址计算都使用 64 位。 ISA 开发人员和 CPU 架构师通常会确保符合规范的软件或固件将在使用该规范构建的任何处理器上执行。广泛部署的 ISA 特许经营权通常还确保向后兼容性，以便为 GenX 版本的处理器编写的代码将继续在 GenX+i 上执行。

大多数现代架构可以归类为基于通用寄存器的加载存储架构，其中明确指定了操作数，并且仅使用加载和存储指令访问内存。除了提供 ISA 中的基本功能（例如使用整数和浮点的加载、存储、控制、标量算术运算）外，广泛部署的架构还在继续增强其 ISA 以支持新的计算范式。这些包括增强的矢量处理指令（例如，英特尔 AVX2、AVX512、ARM SVE）和矩阵/张量指令（英特尔 AMX）。映射为使用这些高级指令的软件通常会在性能上提供数量级的改进。

现代 CPU 支持算术运算的 32b 和 64b 精度。随着深度学习领域的快速发展，业界对变量的替代数字格式重新产生了兴趣，以推动显着的性能改进。研究表明，深度学习模型表现同样出色，使用更少的位来表示变量，节省了计算和内存带宽。因此，除了传统的 32 位之外，一些 CPU 特许经营权最近在 ISA 中增加了对较低精度数据类型的支持，例如 8 位整数（int8，例如 Intel VNNI）、16b 浮点（fp16、bf16）和用于算术运算的 64 位格式。

## Pipelining

流水线是用于使 CPU 快速运行的基础技术，其中多条指令在执行期间重叠。 CPU 中的流水线从汽车装配线中汲取灵感。指令的处理分为几个阶段。这些阶段并行运行，处理不同指令的不同部分。 DLX 是由 [@Hennessy] 定义的简单 5 阶段管道的示例，包括：

1. 取指令（IF）
2.指令解码（ID）
3.执行（EXE）
4. 内存访问（MEM）
5. 回写（WB）

![简单的5级流水线图。](/uarch/Pipelining.png){#fig:Pipelining width=70%}

图@fig:Pipelining 展示了 5 级流水线 CPU 的理想流水线视图。在周期 1 中，指令 x 进入流水线的 IF 阶段。在下一个周期，随着指令 x 移动到 ID 阶段，程序中的下一条指令进入 IF 阶段，依此类推。一旦流水线满了，就像上面的第 5 周期一样，CPU 的所有流水线阶段都忙于处理不同的指令。如果没有流水线，指令 `x+1` 无法开始执行，直到指令 `x` 完成其工作。

大多数现代 CPU 都是深度流水线的，也就是超级流水线。流水线 CPU 的吞吐量定义为每单位时间完成和退出流水线的指令数。任何给定指令的延迟是流水线所有阶段的总时间。由于流水线的所有阶段都链接在一起，因此每个阶段都必须准备好以锁步方式移动到下一条指令。将一条指令从一个阶段移动到另一个阶段所需的时间定义了 CPU 的基本机器周期或时钟。为给定流水线选择的时钟值由流水线的最慢级定义。 CPU 硬件设计人员努力平衡一个阶段可以完成的工作量，因为这直接定义了 CPU 的操作频率。增加频率可以提高性能，并且通常涉及平衡和重新流水线以消除由最慢的流水线阶段引起的瓶颈。

在完美平衡且不会发生任何停顿的理想流水线中，流水线机器中每条指令的时间由下式给出
$$
\textrm{流水线机器上每条指令的时间} = \frac{\textrm{非流水线机器上每条指令的时间}}{\textrm{管道阶段数}}
$$
在实际实现中，流水线引入了几个限制上述理想模型的约束。流水线危害会阻止理想的流水线行为，从而导致停顿。三类风险是结构风险、数据风险和控制风险。对于程序员来说幸运的是，在现代 CPU 中，所有类别的危险都由硬件处理。

* **结构性危害**是由资源冲突引起的。在很大程度上，它们可以通过复制硬件资源来消除，例如使用多端口寄存器或存储器。然而，就硅面积和功率而言，消除所有这些危险可能会变得非常昂贵。

* **数据危害**是由程序中的数据依赖引起的，分为三类：

  *Read-after-write* (RAW) 危险需要依赖读取才能在写入后执行。当指令 x+1 在前一条指令 x 写入源之前读取源时会发生这种情况，从而导致读取错误的值。 CPU 将数据从管道的后期阶段转发到早期阶段（称为“*绕过*”），以减轻与 RAW 危害相关的惩罚。这个想法是指令 x 的结果可以在指令 x 完全完成之前转发到指令 x+1。如果我们看一下这个例子：

  ```
  R1 = R0 加 1
  R2 = R1 加 2
  ```

寄存器 R1 存在 RAW 依赖关系。如果我们在 `R0 ADD 1` 完成后直接取值（来自`EXE`管道阶段），我们不需要等到`WB`阶段完成，值将写入寄存器文件.旁路有助于节省几个周期。管道越长，旁路就越有效。

  *Write-after-read* (WAR) 危险需要依赖写入才能在读取后执行。当指令 x+1 在前一条指令 x 读取源之前写入源时，就会发生这种情况，从而导致读取错误的新值。 WAR 危险不是真正的依赖关系，可以通过称为 [寄存器重命名](https://en.wikipedia.org/wiki/Register_renaming)[^1] 的技术消除。它是一种从物理寄存器中抽象出逻辑寄存器的技术。 CPU 通过保留大量物理寄存器来支持寄存器重命名。由 ISA 定义的逻辑（架构）寄存器只是更广泛的寄存器文件的别名。通过 [架构状态](https://en.wikipedia.org/wiki/Architectural_state)[^3] 的这种解耦，解决 WAR 危害很简单；我们只需要使用不同的物理寄存器进行写操作。例如：

  ```
  R1 = R0 加 1
  R0 = R2 加 2
  ```

  寄存器 R0 存在 WAR 依赖关系。由于我们有大量的物理寄存器，我们可以简单地从写操作开始重命名所有出现的“R0”寄存器。一旦我们通过重命名寄存器“R0”消除了 WAR 危险，我们就可以安全地以任意顺序执行这两个操作。

  *Write-after-write* (WAW) 危害需要依赖写入才能在写入后执行。当指令 x+1 在指令 x 写入源之前写入源时，会发生这种情况，从而导致写入顺序错误。 WAW 危险也通过寄存器重命名消除，允许两个写入以任何顺序执行，同时保留正确的最终结果。

* **控制风险**是由于程序流程的变化引起的。它们来自流水线分支和其他改变程序流程的指令。确定分支方向（采用与未采用）的分支条件在执行管道阶段解决。因此，除非消除了控制风险，否则无法流水线获取下一条指令。下一节中描述的动态分支预测和推测执行等技术用于克服控制风险。

## Exploiting Instruction Level Parallelism (ILP)

程序中的大多数指令都可以流水线化并并行执行，因为它们是独立的。 现代 CPU 实现了大量附加硬件功能，以利用这种指令级并行 (ILP)。 这些硬件功能与高级编译器技术协同工作，可显着提高性能。

### OOO Execution

图@fig:Pipelining 中的流水线示例显示了所有指令按顺序通过流水线的不同阶段，即与它们在程序中出现的顺序相同。大多数现代 CPU 支持乱序 (OOO) 执行，即顺序指令可以以任意顺序进入执行流水线阶段，仅受其依赖关系的限制。 OOO 执行 CPU 仍然必须给出相同的结果，就好像所有指令都按程序顺序执行一样。一条指令在最终执行时称为 *retired*，其结果在 [架构状态] (https://en.wikipedia.org/wiki/Architectural_state) 中是正确且可见的。为确保正确性，CPU 必须按程序顺序退出所有指令。 OOO 主要用于避免由于依赖关系导致的停顿而导致 CPU 资源的未充分利用，尤其是在下一节中描述的超标量引擎中。

这些指令的动态调度是通过复杂的硬件结构（如记分板）和寄存器重命名等技术来实现的，以减少数据危害。 [Tomasulo 算法](https://en.wikipedia.org/wiki/Tomasulo_algorithm)[^4] 在 IBM360 中实现和 [计分板](https://en.wikipedia.org/wiki/Scoreboard)[^5] 1960 年代在 CDC6600 中实现的开创性工作是支持影响所有现代 CPU 架构的动态调度和乱序执行。记分牌硬件用于安排按顺序退役和所有机器状态更新。它跟踪每条指令的数据依赖关系以及数据在管道中的可用位置。大多数实现都力求平衡硬件成本和潜在回报。通常，记分板的大小决定了硬件可以在多远之前寻找调度此类独立指令。

![乱序执行的概念。](/uarch/OOO.png){#fig:OOO width=80%}

图@fig:OOO 用一个例子详细说明了乱序执行的概念。假设由于某些冲突，指令 x+1 无法在周期 4 和 5 中执行。有序 CPU 会阻止所有后续指令进入 EXE 流水线阶段。在OOO CPU中，后续没有任何冲突的指令（例如指令x+2）可以进入并完成其执行。所有指令仍按顺序退出，即指令按程序顺序完成 WB 阶段。

### Superscalar Engines and VLIW

大多数现代 CPU 都是超标量的，即它们可以在给定周期内发出多条指令。发布宽度是在同一周期内可以发布的最大指令数。当前一代 CPU 的典型问题宽度范围为 2-6。为了确保正确的平衡，这种超标量引擎还支持多个执行单元和/或流水线执行单元。 CPU 还将超标量功能与深度管道和乱序执行相结合，以提取给定软件的最大 ILP。

![简单的 2 路超标量 CPU 的流水线图。](/uarch/SuperScalar.png){#fig:SuperScalar width=55%}

图@fig:SuperScalar 显示了一个支持 2 宽问题宽度的 CPU 示例，即在每个周期中，在流水线的每个阶段处理两条指令。超标量 CPU 通常支持多个独立的执行单元，以保持流水线中的指令流过而不会发生冲突。与图@fig:Pipelining 所示的简单流水线处理器相比，复制的执行单元增加了机器的吞吐量。

诸如 Intel Itanium 之类的架构使用称为 VLIW（超长指令字）的技术将调度超标量、多执行单元机器的负担从硬件转移到了编译器。基本原理是通过要求编译器选择正确的指令组合以保持机器充分利用来简化硬件。编译器可以使用软件流水线、循环展开等技术来寻找比硬件结构合理支持的更远的地方来找到正确的 ILP。

### Speculative Execution {#sec:SpeculativeExec}

如上一节所述，如果指令在分支条件得到解决之前停止，则控制风险可能会导致流水线中的显着性能损失。避免这种性能损失的一种技术是硬件分支预测逻辑来预测分支的可能方向并允许从预测路径执行指令（推测执行）。

让我们考虑@lst:Speculative 中的一个简短代码示例。为了让处理器了解它接下来应该执行哪个函数，它应该知道条件 `a < b` 是假还是真。在不知道这一点的情况下，CPU 一直等待直到确定分支指令的结果，如图 @fig:NoSpeculation 所示。

清单：投机执行

~~~~ {#lst:推测的.cpp}
如果 (a < b)
  富（）；
别的
  酒吧（）;
～～～～～～～～～～～～～～～～～～～～～～～～～～～～～～～～～～～～～～～～～～～～～～～～～～～～～～～～～～～～～～～～～～～～～～～～～～～～～～～～～～～～～～～～～～～～～～～～～～～～～～～～～～～～～～～～～～～

<div id="fig:推测">
![无推测](/uarch/Speculative1.png){#fig:NoSpeculation width=60%}


![推测执行](/uarch/Speculative2.png){#fig:SpeculativeExec width=60%}

投机执行的概念。
</div>

通过推测执行，CPU 对分支的结果进行猜测，并从所选路径启动处理指令。假设处理器预测条件“a < b”将被评估为真。它在不等待分支结果的情况下继续进行，并推测性地调用了函数 `foo`（参见图 @fig:SpeculativeExec，推测性工作用`*`标记）。在解决条件以确保机器的体系结构状态永远不会受到推测性执行指令的影响之前，无法提交对机器的状态更改。在上面的示例中，分支指令比较两个标量值，速度很快。但实际上，分支指令可能依赖于从内存加载的值，这可能需要数百个周期。如果预测结果是正确的，它会节省很多周期。但是，有时预测不正确，应该调用函数 `bar`。在这种情况下，推测执行的结果必须被压扁并丢弃。这称为分支误预测惩罚，我们将在 [@sec:BbMisp] 中讨论。

为了跟踪推测的进度，CPU 支持一种称为重新排序缓冲区 (ROB) 的结构。 ROB 维护所有指令执行的状态并按顺序退出指令。推测执行的结果将写入 ROB 并提交给架构寄存器，其顺序与程序流程相同，并且仅在推测正确的情况下。 CPU 还可以将推测执行与乱序执行结合起来，并使用 ROB 来跟踪推测和乱序执行。

## Exploiting Thread Level Parallelism

前面描述的技术依赖于程序中可用的并行性来加速执行。 此外，CPU 支持利用 CPU 上执行的进程和/或线程之间的并行性的技术。 硬件多线程 CPU 支持专用硬件资源来独立跟踪 CPU 中每个线程的状态（也称为上下文），而不是仅跟踪单个执行线程或进程的状态。 这种多线程 CPU 的主要动机是当线程由于长延迟活动（例如内存引用）而被阻塞时，以最小的延迟（不会产生保存和恢复线程上下文的成本）从一个上下文切换到另一个上下文。

### Simultaneous Multithreading

现代 CPU 通过支持同时多线程来结合 ILP 技术和多线程，以从可用硬件资源中获得最大效率。来自多个线程的指令在同一周期内同时执行。从多个线程同时调度指令增加了利用可用超标量资源的可能性，从而提高了 CPU 的整体性能。为了支持 SMT，CPU 必须复制硬件来存储线程状态（程序计数器、寄存器）。跟踪 OOO 和推测执行的资源可以跨线程复制或分区。通常，缓存资源在硬件线程之间动态共享。现代多线程 CPU 支持两个线程 (SMT2) 或四个线程 (SMT4)。

## 内存层次结构 {#sec:MemHierar}

为了有效利用 CPU 中配置的所有硬件资源，需要在正确的时间为机器提供正确的数据。了解内存层次结构对于提供 CPU 的性能能力至关重要。大多数节目都表现出地方性；他们不会统一访问所有代码或数据。 CPU 内存层次结构基于两个基本属性：

* **时间局部性**：当访问给定的内存位置时，很可能在不久的将来再次访问相同的位置。理想情况下，我们希望下次需要此信息时将其保存在缓存中。
* **空间局部性：**当访问给定的内存位置时，很可能在不久的将来会访问附近的位置。这是指将相关数据彼此靠近放置。当程序从内存中读取单个字节时，通常会获取更大的内存块（缓存行），因为通常程序很快就会需要该数据。

本节总结了现代 CPU 支持的内存层次系统的关键属性。

### Cache Hierarchy

缓存是从 CPU 流水线发出的任何请求（针对代码或数据）的内存层次结构的第一级。理想情况下，管道在具有最小访问延迟的无限高速缓存中表现最佳。实际上，任何高速缓存的访问时间都随着大小的增加而增加。因此，高速缓存被组织为最接近执行单元的小型快速存储块的层次结构，并由较大、较慢的块支持。缓存层次结构的特定级别可以专门用于代码（指令缓存，i-cache）或数据（数据缓存，d-cache），或在代码和数据之间共享（统一缓存）。此外，层次结构的某些级别可以专用于特定 CPU，而其他级别可以在 CPU 之间共享。

缓存被组织为具有定义块大小（**缓存行**）的块。现代 CPU 中的典型高速缓存行大小为 64 字节。最接近执行管道的缓存大小通常在 8KiB 到 32KiB 之间。在现代 CPU 中，层次结构中更远的缓存可以是 64KiB 到 16MiB。任何级别的缓存的体系结构都由以下四个属性定义。

#### Placement of data within the cache. 

请求的地址用于访问缓存。在直接映射缓存中，给定的块地址只能出现在缓存中的一个位置，并由如下所示的映射函数定义。
$$
\textrm{缓存中的块数} = \frac{\textrm{缓存大小}}{\textrm{缓存块大小}}
$$
$$
\textrm{直接映射位置} = \textrm{（块地址）mod（缓存中的块数）}
$$

在完全关联的缓存中，给定的块可以放置在缓存中的任何位置。

直接映射和完全关联映射之间的中间选项是集合关联映射。在这样的高速缓存中，块被组织为集合，通常每个集合包含 2,4 或 8 个块。给定地址首先映射到一个集合。在一个集合中，地址可以放置在该集合中的块之间的任何位置。每组具有 m 个块的高速缓存被描述为 m 路组关联高速缓存。组关联缓存的公式是：
$$
\textrm{缓存中的集合数} = \frac{\textrm{缓存中的块数}}{\textrm{每组的块数（关联性）}}
$$
$$
\textrm{Set (m-way) associative location} = \textrm{(block address) mod (Number of Sets in the Cache)}
$$

#### Finding data in the cache.

m 路组关联缓存中的每个块都有一个与之关联的地址标记。此外，标签还包含有效位等状态位，用于指示数据是否有效。标签还可以包含附加位来指示访问信息、共享信息等，这些将在后面的部分中进行描述。

![缓存查找的地址组织。](/uarch/CacheLookup.png){#fig:CacheLookup width=80%}

图@fig:CacheLookup 显示了如何使用从管道生成的地址来检查缓存。最低地址位定义给定块内的偏移量；块偏移位（32 字节缓存线为 5 位，64 字节缓存线为 6 位）。该集合是根据上述公式使用索引位选择的。一旦选择了集合，标签位将用于与该集合中的所有标签进行比较。如果其中一个标签与传入请求的标签匹配并且设置了有效位，则缓存命中结果。与该块条目相关联的数据（与标记查找并行地从缓存的数据数组中读出）被提供给执行管道。在标签不匹配的情况下会发生缓存未命中。

#### 管理失误。

当发生高速缓存未命中时，控制器必须在高速缓存中选择要替换的块来分配导致未命中的地址。对于直接映射缓存，由于只能在单个位置分配新地址，因此之前映射到该位置的条目将被释放，并在其位置安装新条目。在集合关联缓存中，由于新的缓存块可以放置在集合的任何块中，因此需要替换算法。使用的典型替换算法是 LRU（最近最少使用）策略，其中最近最少访问的块被逐出以为未命中地址腾出空间。另一种选择是随机选择一个块作为受害者块。大多数 CPU 在硬件中定义了这些功能，从而更容易执行软件。

#### Managing writes. 

对缓存的读取访问是最常见的情况，因为程序通常读取指令，并且数据读取大于数据写入。处理缓存中的写入更难，CPU 实现使用各种技术来处理这种复杂性。软件开发人员应特别注意硬件支持的各种写入缓存流，以确保其代码的最佳性能。

CPU 设计使用两种基本机制来处理命中缓存的写入：

* 在直写式高速缓存中，命中数据同时写入高速缓存中的块和层次结构的下一个较低级别。
* 在回写式缓存中，命中数据仅写入缓存。随后，层次结构的较低级别包含陈旧的数据。通过标签中的脏位跟踪修改行的状态。当修改后的高速缓存行最终从高速缓存中逐出时，回写操作会强制将数据写回下一个较低级别。

写操作的缓存未命中可以通过两种方式处理：

* 在 *write-allocate 或 fetch on write miss* 缓存中，丢失位置的数据从层次结构的较低级别加载到缓存中，然后像写命中一样处理写操作。
* 如果缓存使用*no-write-allocate policy*，缓存未命中事务直接发送到层次结构的较低级别，并且块不会加载到缓存中。

在这些选项中，大多数设计通常选择使用写分配策略实现回写缓存，因为这两种技术都试图将后续的写事务转换为缓存命中，而不需要额外的流量到层次结构的较低级别。直写缓存通常使用 no-write-allocate 策略。

#### Other cache optimization techniques. 

对于程序员来说，了解缓存层次结构的行为对于从任何应用程序中提取性能至关重要。当 CPU 时钟频率增加而内存技术速度落后时尤其如此。从管道的角度来看，访问任何请求的延迟由以下公式给出，该公式可以递归地应用于缓存层次结构的所有级别，直到主内存：
$$
\textrm{平均访问延迟} = \textrm{命中时间} + \textrm{ 未命中率} \times \textrm{ 未命中惩罚}
$$
硬件设计人员通过许多新颖的微架构技术来应对减少命中时间和未命中损失的挑战。从根本上说，缓存未命中会使流水线停滞并损害性能。任何缓存的未命中率都高度依赖于缓存架构（块大小、关联性）和机器上运行的软件。因此，优化未命中率成为硬件-软件协同设计工作。如前几节所述，CPU 为缓存提供了最佳的硬件组织。下面描述了可以在硬件和软件中实现以最小化高速缓存未命中率的其他技术。

##### 硬件和软件预取。 {#sec:HwPrefetch}

一种减少高速缓存未命中和后续停顿的方法是在流水线需要之前将指令和数据预取到高速缓存层次结构的不同级别。假设是，如果预取请求在管道中足够早地发出，则处理未命中惩罚的时间几乎可以隐藏。大多数 CPU 支持基于硬件的隐式预取，并辅以程序员可以控制的显式软件预取。

硬件预取器观察正在运行的应用程序的行为，并在缓存未命中的重复模式上启动预取。硬件预取可以自动适应应用程序的动态行为，例如变化的数据集，并且不需要优化编译器的支持或分析支持。此外，硬件预取不需要额外的地址生成和预取指令的开销。然而，硬件预取仅限于对在硬件中实现的一组有限的缓存未命中模式进行学习和预取。

软件存储器预取补充了由硬件完成的预取。开发人员可以通过专用的硬件指令提前指定需要哪些内存位置（参见 [@sec:memPrefetch]）。编译器还可以自动将预取指令添加到代码中，以便在需要数据之前请求数据。预取技术需要在需求和预取请求之间取得平衡，以防止预取流量减慢需求流量。

### Main Memory

主存储器是层次结构的下一级，位于缓存的下游。主存储器使用 DRAM（动态 RAM）技术，以合理的成本支持大容量。主存储器由三个主要属性描述 - 延迟、带宽和容量。延迟通常由两个组件指定。内存访问时间是从请求到数据字可用之间经过的时间。内存循环时间定义了两次连续访问内存之间所需的最短时间。

DDR（双倍数据速率）DRAM 技术是大多数 CPU 支持的主要 DRAM 技术。从历史上看，DRAM 带宽每一代都在提高，而 DRAM 延迟保持不变甚至增加。 @tbl:mem_rate 表显示了过去三代 DDR 技术的最高数据速率和相应的延迟。数据速率以每秒一百万次传输 (MT/s) 来衡量。此表中显示的延迟对应于 DRAM 设备本身的延迟。通常，由于缓存控制器、内存控制器和片上互连中产生的额外延迟和排队延迟，从 CPU 流水线中看到的延迟（要使用的负载上的缓存未命中）更高（在 70ns-150ns 范围内） . 

----------------------------------------
   DDR      Highest Data   Typical Read 
Generation   Rate (MT/s)   Latency (ns)
----------  ------------   -------------
  DDR3          2133          10.3

  DDR4          3200          12.5

  DDR5          6400          14

----------------------------------------

表：最近三代 DDR 技术的最高数据速率和相应的延迟。 {#tbl:mem_rate}

新的 DRAM 技术，如 GDDR（图形 DDR）和 HBM（高带宽内存）被需要更高带宽的定制处理器使用，而 DDR 接口不支持。

现代 CPU 支持多个独立的 DDR DRAM 内存通道。通常，每个内存通道都是 32 位或 64 位宽。

＃＃ 虚拟内存

虚拟内存是与 CPU 上执行的所有进程共享连接到 CPU 的物理内存的机制。虚拟内存提供了一种保护机制，限制其他进程访问分配给给定进程的内存。虚拟内存还提供重定位，即在物理内存中的任何位置加载程序而不更改程序中的寻址的能力。

在支持虚拟内存的 CPU 中，程序使用虚拟地址进行访问。这些虚拟地址由提供虚拟地址和物理地址之间映射的专用硬件表转换为物理地址。这些表称为页表。地址转换机制如下所示。虚拟地址分为两部分。虚拟页号用于索引到页表（页表可以是单层也可以是嵌套的），以产生虚拟页号和对应的物理页之间的映射。然后使用与虚拟地址的页面偏移量来访问映射物理页面中相同偏移量的物理内存位置。如果请求的页面不在主存储器中，则会导致页面错误。操作系统负责向硬件提供提示以处理页面错误，以便可以换出最近最少使用的页面之一，以便为新页面腾出空间。

![缓存查找的地址组织。](/uarch/VirtualMem.png){#fig:VirtualMem width=70%}

CPU 通常使用分层页表格式将虚拟地址位有效地映射到可用的物理内存。在这样的系统中，页面丢失会很昂贵，需要遍历层次结构。为了减少地址转换时间，CPU 支持一种称为转换后备缓冲区 (TLB) 的硬件结构来缓存最近使用的转换。

## SIMD Multiprocessors {#sec:SIMD}

与上一节中描述的 MIMD 方法相比，另一种广泛用于某些工作负载的多处理变体称为 SIMD（单指令多数据）多处理器。顾名思义，在 SIMD 处理器中，一条指令通常使用许多独立的功能单元在一个周期内对许多数据元素进行操作。向量和矩阵的科学计算非常适合 SIMD 架构，因为向量或矩阵的每个元素都需要使用相同的指令进行处理。 SIMD 多处理器主要用于此类特殊用途的任务，这些任务是数据并行的并且只需要一组有限的功能和操作。

图@fig:SIMD 显示了@lst:SIMD 中列出的代码的标量和SIMD 执行模式。在传统的 SISD（单指令，单数据）模式中，加法运算分别应用于数组 `a` 和 `b` 的每个元素。但是，在 SIMD 模式下，会同时对多个元素应用加法。 SIMD CPU 支持能够对向量元素执行不同操作的执行单元。数据元素本身可以是整数或浮点数。 SIMD 架构允许更高效地处理大量数据，并且最适合涉及矢量运算的数据并行应用程序。

Listing: SIMD execution

~~~~ {#lst:SIMD .cpp}
double *a, *b, *c;
for (int i = 0; i < N; ++i) {
  c[i] = a[i] + b[i];
}
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

![Example of scalar and SIMD operations.](/uarch/SIMD.png){#fig:SIMD width=80%}

大多数流行的 CPU 架构都具有向量指令，包括 x86、PowerPC、ARM 和 RISC-V。 1996 年 Intel 发布了一个新的指令集 MMX，它是一种专为多媒体应用设计的 SIMD 指令集。 继 MMX 之后，英特尔推出了具有更多功能和更大矢量大小的新指令集：SSE、AVX、AVX2、AVX512。 新指令集一经推出，软件工程师就可以开始使用它们。 起初，新的 SIMD 指令是用汇编语言编写的。 后来，引入了特殊的编译器内在函数。 今天，所有主要编译器都支持流行处理器的矢量化。

[^1]: Register renaming - [https://en.wikipedia.org/wiki/Register_renaming](https://en.wikipedia.org/wiki/Register_renaming).

[^3]: Architectural state - [https://en.wikipedia.org/wiki/Architectural_state](https://en.wikipedia.org/wiki/Architectural_state).
[^4]: Tomasulo algorithm - [https://en.wikipedia.org/wiki/Tomasulo_algorithm](https://en.wikipedia.org/wiki/Tomasulo_algorithm).
[^5]: Scoreboarding - [https://en.wikipedia.org/wiki/Scoreboarding](https://en.wikipedia.org/wiki/Scoreboarding).

